{"nbformat_minor": 2, "cells": [{"source": "# VI - Automatic Model Selection from Parametric Sweep using Task Dependencies\nIn this notebook we will be taking the example from the [Parametric Sweep](04_Parameter_Sweep.ipynb) notebook and automating the entire chain using task dependencies in a single Azure Batch job.\n\n* [Setup](#section1)\n* [Configure job](#section2)\n* [Submit job](#section3)\n* [Download best model](#section4)\n* [Delete job](#section5)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section1'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Setup", "cell_type": "markdown", "metadata": {}}, {"source": "Create a simple alias for Batch Shipyard", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Check that everything is working", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Read in the account information we saved earlier", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import json\n\ndef read_json(filename):\n    with open(filename, 'r') as infile:\n        return json.load(infile)\n    \naccount_info = read_json('account_information.json')\n\nstorage_account_key = account_info['storage_account_key']\nstorage_account_name = account_info['storage_account_name']", "outputs": [], "metadata": {"collapsed": true}}, {"source": "<a id='section2'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Configure Job", "cell_type": "markdown", "metadata": {}}, {"source": "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. The difference here is that depending on the number of combinations we will be creating the same number of tasks. Each task will have a different set of parmeters that we will be passing to our model training script. This parameters effect the training of the model and in the end the performance of the model. The model and results of its evaluation are recorded and stored on the node. At the end of the task the results are pulled into the specified storage container.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from copy import copy\nimport os\nimport random\nfrom sklearn.grid_search import ParameterGrid\nfrom toolz import curry, pipe\n\ndef write_json_to_file(json_dict, filename):\n    \"\"\" Simple function to write JSON dictionaries to files\n    \"\"\"\n    with open(filename, 'w') as outfile:\n        json.dump(json_dict, outfile)\n\nCNTK_TRAIN_DATA_FILE = 'Train_cntk_text.txt'\nCNTK_TEST_DATA_FILE = 'Test_cntk_text.txt'\nURL_FMT = 'https://{}.blob.core.windows.net/{}/{}'\n\ndef select_random_data_storage_container():\n    \"\"\"Randomly select a storage account and container for CNTK train/test data.\n    This is specific for the workshop to help distribute attendee load. This\n    function will only work on Python2\"\"\"\n    ss = random.randint(0, 4)\n    cs = random.randint(0, 4)\n    sa = '{}{}bigai'.format(ss, chr(ord('z') - ss))\n    cont = '{}{}{}'.format(cs, chr(ord('i') - cs * 2), chr(ord('j') - cs * 2))\n    return sa, cont\n\ndef create_resource_file_list():\n    sa, cont = select_random_data_storage_container()\n    ret = [{\n        'file_path': CNTK_TRAIN_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TRAIN_DATA_FILE)\n    }]\n    sa, cont = select_random_data_storage_container()\n    ret.append({\n        'file_path': CNTK_TEST_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TEST_DATA_FILE)\n    })\n    return ret\n\ndef compose_command(num_convolution_layers, minibatch_size, max_epochs=30):\n    cmd_str = ' '.join((\"source /cntk/activate-cntk;\",\n                        \"python /code/ConvNet_CIFAR10.py\",\n                        \"--num_convolution_layers {num_convolution_layers}\",\n                        \"--minibatch_size {minibatch_size}\",\n                        \"--max_epochs {max_epochs}\")).format(num_convolution_layers=num_convolution_layers,\n                                                             minibatch_size=minibatch_size,\n                                                             max_epochs=max_epochs)\n    return 'bash -c \"{}\"'.format(cmd_str)\n\n@curry\ndef append_parameter(param_name, param_value, data_dict):\n    data_dict[param_name]=param_value\n    return data_dict", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Create the task template:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "STORAGE_ACCOUNT_ALIAS = \"mystorageaccount\"\n\nIMAGE_NAME = \"masalvar/cntkcifar\" # Custom CNTK image\n\n_TASK_TEMPLATE = {\n    \"image\": IMAGE_NAME,\n    \"remove_container_after_exit\": True,\n    \"gpu\": True,\n    \"output_data\": {\n        \"azure_storage\": [\n            {\n                \"storage_account_settings\": STORAGE_ACCOUNT_ALIAS,\n                \"container\": \"output\",\n                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n            },\n        ]\n    },\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Here we modify the `task_generator` to append an additional parameter to each task definition with a specific `id` numbered from `0`. This is done so we can reference each dependent task in the selection task that must run after each training task has completed successfully.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def task_generator(parameters):\n    id = 0\n    for params in ParameterGrid(parameters):\n        yield pipe(copy(_TASK_TEMPLATE),\n                   append_parameter('command', compose_command(**params)),\n                   append_parameter('resource_files', create_resource_file_list()),\n                   append_parameter('id', str(id)))\n        id += 1", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Generate the `jobs.json` configuration file", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "parameters = {\n    \"num_convolution_layers\": [2, 3, 4],\n    \"minibatch_size\": [32, 64, 128]\n}", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "JOB_ID = 'cntk-ps-as-job'\n\njobs = {\n    \"job_specifications\": [\n        {\n            \"id\": JOB_ID,\n            \"tasks\": list(task_generator(parameters))    \n        }\n    ]\n}\n\nnum_parameter_sweep_tasks = len(jobs['job_specifications'][0]['tasks'])\nprint('number of tasks for parametric sweep {}: {}'.format(JOB_ID, num_parameter_sweep_tasks))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now we'll create the Python program to run that performs the best model selection. Note that this code is nearly similar to the code for selecting the best model locally in the [Parameter sweep notebook](04_Parameter_Sweep.ipynb).", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%writefile autoselect.py\nimport json\nimport os\nimport shutil\n\ndef read_json(filename):\n    with open(filename, 'r') as infile:\n        return json.load(infile)\n\ndef scandir(basedir):\n    for root, dirs, files in os.walk(basedir):\n        for f in files:\n            yield os.path.join(root, f) \n\nMODELS_DIR = os.path.join('wd', 'Models')\n            \nresults_dict = {}\nfor model in scandir(MODELS_DIR):\n    if not model.endswith('.json'):\n        continue\n    key = model.split(os.sep)[2]  # due to MODELS_DIR path change\n    results_dict[key] = read_json(model)\n\n# use items() instead of iteritems() as this will be run in python3\ntuple_min_error = min(results_dict.items(), key=lambda x: x[1]['test_metric'])\nconfiguration_with_min_error = tuple_min_error[0]\nprint('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))\n\n# copy best model to wd\nMODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\nshutil.copy(os.path.join(MODELS_DIR, configuration_with_min_error, MODEL_NAME), '.')", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We now need to prepare the file to be uploaded to the Azure Storage account to be referenced in the task:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "INPUT_CONTAINER = 'input-autoselect'\nOUTPUT_CONTAINER = 'output-autoselect'\nUPLOAD_DIR = 'autoselect_upload'\n\n!rm -rf $UPLOAD_DIR\n!mkdir -p $UPLOAD_DIR\n!mv autoselect.py $UPLOAD_DIR\n!ls -alF $UPLOAD_DIR", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Alias `blobxfer` and upload it to `INPUT_CONTAINER`:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias blobxfer python -m blobxfer", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "blobxfer $storage_account_name $INPUT_CONTAINER $UPLOAD_DIR --upload --storageaccountkey $storage_account_key", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now we'll append the `auto-model-selection` task which depends on the prior training tasks. The important properties here are `depends_on_range` which specifies a range of task ids the `auto-model-selection` task depends on. Additionally, this task requires data from the prior run task which is specified in `input_data`.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def generate_input_data_spec(job_id, task_id):\n    return {\n        \"job_id\": job_id,\n        \"task_id\": task_id,\n        \"include\": [\"wd/Models/*_{}_{}/*\".format(task_id, job_id)]\n    }\n\ninput_data = []\nfor x in range(0, num_parameter_sweep_tasks):\n    input_data.append(generate_input_data_spec(JOB_ID, str(x)))\n\nmodel_selection_task = {\n    \"id\": \"auto-model-selection\",\n    \"command\": \"python autoselect.py\",\n    \"depends_on_range\": [0, num_parameter_sweep_tasks - 1],\n    \"image\": IMAGE_NAME,\n    \"remove_container_after_exit\": True,\n    \"input_data\": {\n        \"azure_batch\": input_data,\n        \"azure_storage\": [\n            {\n                \"storage_account_settings\": STORAGE_ACCOUNT_ALIAS,\n                \"container\": INPUT_CONTAINER\n            }\n        ]\n    },\n    \"output_data\": {\n        \"azure_storage\": [\n            {\n                \"storage_account_settings\": STORAGE_ACCOUNT_ALIAS,\n                \"container\": OUTPUT_CONTAINER,\n                \"include\": [\"*wd/ConvNet_CIFAR10_model.dnn\"],\n                \"blobxfer_extra_options\": \"--delete --strip-components 2\"\n            }\n        ]\n    }\n}\n\n# append auto-model-selection task to jobs\njobs['job_specifications'][0]['tasks'].append(model_selection_task)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\nprint(json.dumps(jobs, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section3'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Submit job\nCheck that everything is ok with our pool before we submit our jobs", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool list", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now that we have confirmed everything is working we can execute our job using the command below. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs add", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "Using the command below we can check the status of our jobs. Once all jobs have an exit code we can continue. You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs listtasks --jobid $JOB_ID", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "<a id='section4'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Download best model\nThe best performing model from the parametric sweep job should now be saved to our `OUTPUT_CONTAINER` container by the `auto-model-selection` task. Let's save this model in `MODELS_DIR`:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "MODELS_DIR = 'auto-selected-model'", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Download the best performing model:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "blobxfer $storage_account_name $OUTPUT_CONTAINER $MODELS_DIR --remoteresource . --download --storageaccountkey $storage_account_key", "outputs": [], "metadata": {"collapsed": false}}, {"source": "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "!ls -alF $MODELS_DIR", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section5'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Delete job", "cell_type": "markdown", "metadata": {}}, {"source": "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs del -y --termtasks --wait", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Next Steps\nYou can proceed to the [Notebook: Clean Up](05_Clean_up.ipynb) if you are done for now, or proceed to one of the following additional Notebooks:\n* [Notebook: Tensorboard Visualization](07_Advanced_Tensorboard.ipynb) - note this requires running this notebook on your own machine\n* [Notebook: Parallel and Distributed](08_Advanced_Parallel_and_Distributed.ipynb)\n* [Notebook: Keras with TensorFlow](09_Keras_Single_GPU_Training_With_Tensorflow.ipynb)", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}, "anaconda-cloud": {}}}