{"nbformat_minor": 2, "cells": [{"source": "# IX - Keras+TensorFlow Single GPU Training\nIn the previous notebook we have been using Microsoft's Cognitive Toolkit (CNTK) as our Deep Learning framework. Batch Shipyard though is not limited to CNTK and can be used with any Deep Learning framework. A very popular combination is TensorFlow and Keras. Keras is an easy to use high level API for TensorFlow and Theano, which makes creating artificial neural networks easy. In this notebook we will quickly demonstrate how to train a CNN using Keras on Batch Shipyard. \n\n* [Setup](#section1)\n* [Define our model](#section2)\n* [Configure and create pool](#section3)\n* [Configure job](#section4)\n* [Submit job](#section5)\n* [Delete job](#section6)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section1'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Setup", "cell_type": "markdown", "metadata": {}}, {"source": "Create a simple alias for Batch Shipyard", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Check that everything is working", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section2'><\/a>\n## Define Our Model\nThe file below contains a simple CNN written in Keras. It will load the CIFAR 10 data and then train the model for a number of epochs and then evaludate it on the test set.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%writefile cifar10_cnn.py\n'''Train a simple deep CNN on the CIFAR10 small images dataset.\n'''\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nbatch_size = 32\nnum_classes = 10\nepochs = 20\ndata_augmentation = True\n\n# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test))\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section3'><\/a>\n## Configure  and Create Pool\nHere we will simply be using a pool with a single node just as a demonstration. Upon pool creation Batch Shipyard will pull our model file above and place into the blob container **input**.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "IMAGE_NAME = \"masalvar/keras\" # Keras image", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "INPUT_CONTAINER = 'input'\nUPLOAD_DIR = 'dist_upload'\n\n!rm -rf $UPLOAD_DIR\n!mkdir -p $UPLOAD_DIR\n!mv cifar10_cnn.py $UPLOAD_DIR\n!ls -alF $UPLOAD_DIR", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "STORAGE_ALIAS = 'mystorageaccount'\n\nconfig = {\n    \"batch_shipyard\": {\n        \"storage_account_settings\": STORAGE_ALIAS\n    },\n    \"global_resources\": {\n        \"docker_images\": [\n            IMAGE_NAME\n        ],\n        \"files\": [\n            {\n                \"source\": {\n                    \"path\": UPLOAD_DIR\n                },\n                \"destination\": {\n                    \"storage_account_settings\": STORAGE_ALIAS,\n                    \"data_transfer\": {\n                        \"container\": INPUT_CONTAINER\n                    }\n                }\n            }\n        ]\n    }\n}", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "POOL_ID = 'gpupool-keras'\n\npool = {\n    \"pool_specification\": {\n        \"id\": POOL_ID,\n        \"vm_size\": \"STANDARD_NC6\",\n        \"vm_count\": 1,\n        \"publisher\": \"Canonical\",\n        \"offer\": \"UbuntuServer\",\n        \"sku\": \"16.04-LTS\",\n        \"ssh\": {\n            \"username\": \"docker\"\n        },\n        \"reboot_on_start_task_failed\": False,\n        \"block_until_all_global_resources_loaded\": True,\n        \"input_data\": {\n            \"azure_storage\": [\n                {\n                    \"storage_account_settings\": STORAGE_ALIAS,\n                    \"container\": INPUT_CONTAINER,\n                    \"destination\": \"$AZ_BATCH_NODE_SHARED_DIR/code\"\n                }\n            ]\n        },\n        \"transfer_files_on_pool_creation\": True,\n    }\n}", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "import json\nimport os\n\ndef write_json_to_file(json_dict, filename):\n    \"\"\" Simple function to write JSON dictionaries to files\n    \"\"\"\n    with open(filename, 'w') as outfile:\n        json.dump(json_dict, outfile)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "write_json_to_file(config, os.path.join('config', 'config.json'))\nwrite_json_to_file(pool, os.path.join('config', 'pool.json'))\nprint(json.dumps(config, indent=4, sort_keys=True))\nprint(json.dumps(pool, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Allocate the pool, please be patient for this step.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool add -y", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section4'><\/a>\n## Configure Job\nAs before the dictionary below defines the job we will execute. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "TASK_ID = 'run_cifar10' # This should be changed per task\n\nJOB_ID = 'keras-training-job'\n\nCOMMAND = 'bash -c \"python -u $AZ_BATCH_NODE_SHARED_DIR/code/cifar10_cnn.py\"'\n\nOUTPUT_STORAGE_ALIAS = \"mystorageaccount\"\n\njobs = {\n    \"job_specifications\": [\n        {\n            \"id\": JOB_ID,\n            \"tasks\": [\n                {\n                    \"id\": TASK_ID,\n                    \"image\": IMAGE_NAME,\n                    \"remove_container_after_exit\": True,\n                    \"command\": COMMAND,\n                    \"gpu\": True,\n                }\n            ],\n        }\n    ]\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Write the jobs configuration to the `jobs.json` file:", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\nprint(json.dumps(jobs, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section5'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Submit Job\nCheck that everything is ok with our pool before we submit our jobs.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool listnodes", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now that we have confirmed everything is working we can execute our job using the command below. The tail switch at the end will stream stdout from the node.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs add --tail stdout.txt", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "If something goes wrong you can run the following command to get the stderr output from the job.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard data stream --filespec $JOB_ID,$TASK_ID,stderr.txt", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section6'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Delete job", "cell_type": "markdown", "metadata": {}}, {"source": "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs del -y --termtasks --wait", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "## Next Steps\nYou can proceed to the [Notebook: Clean Up](05_Clean_up.ipynb) if you are done for now, or proceed to one of the following additional Notebooks:\n* [Notebook: Automatic Model Selection](06_Advanced_Auto_Model_Selection.ipynb)\n* [Notebook: Tensorboard Visualization](07_Advanced_Tensorboard.ipynb) - note this requires running this notebook on your own machine\n* [Notebook: Parallel and Distributed](08_Advanced_Parallel_and_Distributed.ipynb)", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}